---
layout: post
title: 穿过丛林，来看看Kubernetes组件(pods, rc, services, volumes...)
date: 2017-11-05
tags: Kubernetes
---

# 第二式： 认知Kubernetes组件

## 目标：
* 学会Kubernetes是如何调度集群的
* 认知pods概念和如何使用pods
* 认知replication controller概念和如何使用rs
* 认知services概念和如何使用services
* 认知volumes概念和如何使用volumes
* 认知secrets概念和如何使用secrets
* 认知names概念和如何使用names
* 认知labels和selectors的作用和如何使用labels和selectors

## 介绍：
> 仔细的阅读和完成以下内容，你将会理解Kubernetes各个主要资源的概念和如何使用他们，如pod，service， volume等。你将会体验到kubernetes的强大，可扩展的，高可用，高效的...... 于此同时，你可能会明白微服务的持续交付流程；

#### 如何操纵kubernetes集群？
1. 通过kubectl命令接口: 

   > 1 . kubectl通过RESTful api的方式连接到API服务器,默认情况下访问localhost，你可以通过**--server** 选项去更改API服务器的地址；
   
   > 2 . kubectl允许通过命令行参数的形式创建资源，也可以通过 **-f** 接收外来文件的方式(支持json或yaml格式)创建资源；为了更好的维护集群，建议使用文件的方式；
   
   > 3 . 命令格式： kubectl [command] [type] [name] [flags]
   >> command: 执行你想要执行的操作，即action，比如run, delete, start, stop；
   <br>
   >> type: 指定资源类型, 比如pod, services，区分大小写，支持简写,比如replication controller可以写成rs；
   <br>
   >> name: 指定资源的name， 区分大小写；
   <br>
   >> flags: 其它的参数;
   <br>
   >> 例子： kubectl run my-first-nginx --image=nginx replicationcontroller "my-first-nginx"
   
2. 通过RESTful API

## names

#### 什么是names？

###### 当我们在创建kubernetes资源的时候，	我们需要指定一个name，这个name在kubernetes是唯一的；

###### kubernetes对name的限定如下：

1. 最大253个字符
2. 小写字母和数字
3. 可以使用特殊字符(-)和（.）


## Labels和Selectors

#### 什么是Label？

###### Labels是一组key-value的键值对，会存放在对象的元数据(metadata)当中，我们可以使用labels去将资源进行选择，组织和分类，资源可以是pods，replication controllers和services等，labels不需要是独一无二的；

##### 一个有效的名字规则如下：
* 名字和可选的前缀名称，用斜杠分隔
* 前缀必须是一个DNS的子域名，用"."分割，不能超过253个字符
* 名字必须少于63个字符，\[a-z0-9A-Z],可以使用特殊字符“~”,”_“,”,“;

##### 一个有效的value规则如下:
* 必须少于63个字符，\[a-z0-9A-Z],可以使用特殊字符“~”,”_“,”,“;



#### 什么是selector？

###### 是kubernetes核心的分组机制，通过label selector，客户端或用户能够识别一组具有共同特征或属性的kubernetes对象，一个label selector可以由多个查询条件组成，这些查询条件用逗号分隔，逗号起到逻辑与的关系，目前支持两种类型的查询

  1. 基于值相等的查询条件：[=, ==, !=], [environment = production;  tier !=frontend;  environment=production,tier !=frontend]
  2. 基于子集的查询条件: [in, notin, exists]

#### 让我们来实际操作一把

```
# 1. 创建一个标签,想问名字为lambert，环境为staging，层次为frontend的pod
[root@srv-k8s-master1 ~]# cat label-nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    project: lambert
    environment: staging
    tier: frontend
spec:
  containers:
    -
      image: nginx
      imagePullPolicy: IfNotPresent
      name: nginx
      ports:
      - containerPort: 80
 
＃ 2. 创建一个名字为nginx-pod的deployment，标签环境为production, 项目名字为pilot，层次为frontend 
[root@srv-k8s-master1 ~]# kubectl run nginx-pod --image=nginx --replicas=2 --port=80 --labels="environment=production,project=pilot,tier=frontend"
deployment "nginx-pod" created

＃ 3. 根据标签，获取pod
[root@srv-k8s-master1 ~]# kubectl get pods -l "project=pilot,environment=production"
NAME                         READY     STATUS    RESTARTS   AGE
nginx-pod-2331167648-448lz   1/1       Running   0          52s
nginx-pod-2331167648-7wjw0   1/1       Running   0          52s
```

#### 还有，在某种场景，我们想给资源添加一些标签，这些标签只是个参考，比如当前的代码版本，这个该怎么办呢？kubernetes提供了一个叫做annotation的概念，我们可以将其添加到要创建资源的metadata当中, 我们也可以通过volume的downwardAPI挂载到容器当中；

```
[root@srv-k8s-master1 ~]# cat annotation-sample.yaml
apiVersion: v1
kind: Pod
metadata:
  name: annotations-example
  labels:
    app: nginx
  annotations:
    container: nginx

spec:
  containers:
  - name: annotations-example
    image: nginx
    ports:
    - containerPort: 80

[root@srv-k8s-master1 ~]# kubectl create -f annotation-sample.yaml
pod "annotations-example" created
```
  

## namespaces:

#### 什么是namespace？

###### 资源的名称是kubernetes集群中命名空间内的唯一标识符，使用kubernets命名空间可以隔离同一集群中不同环境的命名空间， 借助kubernetes namespace，我们可以将资源分配给不同的项目和团队；

###### Pods, services, replication controllers是在某个命名空间当中，一些资源，比如nodes和PVs，不属于任何命名空间；

#### 默认情况下，kubernetes创建了一个默认的命名空间，所有的对象都属于default的命名空间，
kubernetes也会创建一个叫做kube-system的命名空间，用来存放kubernete系统本身的对象；

```
[root@srv-k8s-master1 ~]# kubectl get namespaces
NAME          STATUS    AGE
default       Active    2d
kube-system   Active    2d
```

#### 如何创建新的namespace呢？

>   注意： namespace的名字最多63个字符，匹配\[a-z0-9]([-a-z0-9]*[a-z0-9])

 ```
 # 1. 准备yaml文件
 [root@srv-k8s-master1 ~]# cat newnamespace.yaml
  apiVersion: v1
  kind: Namespace
  metadata:
  name: new-namespace
  
  # 2. 创建
  [root@srv-k8s-master1 ~]# kubectl create -f newnamespace.yaml
  namespace "new-namespace" created
  
  # 3. 获取namespace
  [root@srv-k8s-master1 ~]# kubectl get namespaces
  NAME            STATUS    AGE
  default         Active    2d
  kube-system     Active    2d
  new-namespace   Active    7s
  
  # 4. 创建新的pod在默认的命名空间和新的命名空间
  [root@srv-k8s-master1 ~]# kubectl run nginx --image=nginx --namespace=new-namespace
   deployment "nginx" created
  [root@srv-k8s-master1 ~]# kubectl run nginx --image=nginx
   deployment "nginx" created
   
   # 5. 获取pod
   [root@srv-k8s-master1 ~]# kubectl get pods --all-namespaces | grep nginx
	default         nginx-701339712-1f4p2                   1/1       Running            0          1m
	new-namespace   nginx-701339712-svhwp                   1/1       Running            0          2m
	
	# 6. 查看当前的上下文环境
	[root@srv-k8s-master1 ~]# kubectl config view
	apiVersion: v1
	clusters: []
	contexts: []
	current-context: ""
	kind: Config
	preferences: {}
	users: []
	
	# 7. 创建新的上下文环境
	[root@srv-k8s-master1 ~]# kubectl config set-context new-context --namespace=new-namespace
	Context "new-context" set.
	
	[root@srv-k8s-master1 ~]# kubectl config view
	apiVersion: v1
	clusters: []
	contexts:
	- context:
	    cluster: ""
	    namespace: new-namespace
	    user: ""
	  name: new-context
	current-context: ""
	kind: Config
	preferences: {}
	users: []
	
	# 8. 切换到新的上下文环境
	[root@srv-k8s-master1 ~]# kubectl config use-context new-context
	Switched to context "new-context".
	
	# 9. 检查是否切换成功
	[root@srv-k8s-master1 ~]# kubectl config view | grep current-context
	current-context: new-context
	
	# 10. 再次检查，发现只有一个pod，通过**--all-namespaces**查看所有
	[root@srv-k8s-master1 ~]# kubectl get pods
	NAME                    READY     STATUS    RESTARTS   AGE
	nginx-701339712-svhwp   1/1       Running   0          10m
 ```
 
 

通过namespace还能做点什么呢？可以做资源限制呀，可以用来限制创建的资源呀
 
 ```
 [root@srv-k8s-master1 ~]# kubectl describe namespaces new-namespace
  Name:	new-namespace
  Labels:	<none>
  Status:	Active
	
  No resource quota.
	
  No resource limits.
 ```
 
 默认是没有资源配额和限制的，Kubernetes支持限制pod或容器，在使用资源限制之前，需要启用Kubernetes API服务器的LimitRanger；
 
 ```
[root@srv-k8s-master1 ~]# cat /etc/kubernetes/apiserver  | grep KUBE_ADMISSION_CONTROL
 KUBE_ADMISSION_CONTROL="--admission-control=LimitRanger"
[root@srv-k8s-master1 ~]# systemctl restart kube-apiserver 
 ```
 
 ```
# 1. 创建资源限制yaml文件 
[root@srv-k8s-master1 ~]# cat resourcelimit.yaml
apiVersion: v1
kind: LimitRange 
metadata:
  name: limits
  namespace: new-namespace
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
 
# 2. 创建    
[root@srv-k8s-master1 ~]# kubectl create -f resourcelimit.yaml
limitrange "limits" created

# 3. 获取限制
[root@srv-k8s-master1 ~]# kubectl get LimitRange
NAME      AGE
limits    1m

# 4. 查看详细限制
[root@srv-k8s-master1 ~]# kubectl describe namespace new-namespace
Name:	new-namespace
Labels:	<none>
Status:	Active

No resource quota.

Resource Limits
 Type		Resource	Min	Max	Default Request	Default Limit	Max Limit/Request Ratio
 ----		--------	---	---	---------------	-------------	-----------------------
 Container	memory		-	-	256Mi		512Mi
		-

# 5. 删除LimitRange
[root@srv-k8s-master1 ~]# kubectl delete LimitRange limits
limitrange "limits" deleted		
		
 ```

## 聊聊Pods
#### 什么是pod呢？

1. 创建，调度以及管理的最小单元，即容器为鸡蛋，pod为篮子，kubernetes将“篮子”进行调度
2. 共存的一组容器的集合
3. 容器共享PID，网络，IPC(网络)以及UTS(主机名)命名空间
4. 容器共享存储卷
5. 短暂存在的对象，包含以下几个状态：“pending, running, success, failed”
6. 同一个pod内部的端口不能重复，同一个宿主机的端口也不能有冲突
7. 通过kubectl get pod，查看创建的pod信息
8. 通过kubectl logs $pod_name $container_name查看container的日志信息
9. 因此之间有耦合关系的容器建议放在一个pod中，比如app和database

#### 实操：

> **注意，以下所有的命令都在kubectl版本为1.5.2上操作, 由于版本不同，操作命令可能不同，具体请参考官网**
<br>

1. 在做之前，确保node节点是ready的状态, 如果是NotReady状态，请检查双方的iptables规则，查看node节点的日志/var/log/message，找到原因；

	```
	[root@srv-k8s-master1 ~]# kubectl get nodes
	NAME            STATUS    AGE
	srv-k8s-node2   Ready     1d
	```

2. 编写pod yaml文件
	
	```
	[root@srv-k8s-master1 ~]# cat my-first.pod.yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: my-first-pod
	spec:
	  containers:
	  - name: my-nginx
	    image: nginx
	  - name: my-centos
	    image: centos
	    command: ["/bin/sh", "-c", "while : ; do curl http://localhost:80/; sleep 3; done"]
	
	```

3. 执行创建操作

	```
	[root@srv-k8s-master1 ~]# kubectl create -f my-first.pod.yaml
    pod "my-first-pod" created
    
	```
	
4. 查看pod状态

   ```
   # kubectl get po
   [root@srv-k8s-master1 ~]# kubectl get pods | grep my-first-pod
    my-first-pod                            2/2       Running            0          4m
    
   ```	
   
5. 检查容器my-centos是否可以访问my-nginx容器， 如下，pod链接两个不同的容器，nginx和centos在同一个linux命名空间中

   ```
   [root@srv-k8s-master1 ~]# kubectl logs my-first-pod -c my-centos --tail=10
<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>
<p><em>Thank you for using nginx.</em></p>
</body>
</html>
100   612  100   612    0     0  95610      0 --:--:-- --:--:-- --:--:--   99k
   ```
 
 > 如果遇到以下类似的报错，是因为master无法访问node节点，然后获取不到对应的信息
 	```
	[root@srv-k8s-master1 ~]# kubectl logs my-first-pod -c my-centos --tail=30
Error from server: Get https://srv-k8s-node2:10250/containerLogs/default/my-first-pod/my-centos?tailLines=30: dial tcp: lookup srv-k8s-node2 on 114.114.114.114:53: no such host
	```   
> 解决方式:
   ```
   [root@srv-k8s-master1 ~]# echo "10.1.96.233 srv-k8s-node2" >>/etc/hosts
   ```

6. 这是怎么工作的呢？
   * master执行创建pod的命令之后，scheduler分发创建的指令给node的kubelet进程，kubelet负责所有的launch操作。

7. 通过 **-o wide**选项用来查看pod在那台node上创建

   ```
   [root@srv-k8s-master1 ~]# kubectl get pods -o wide | grep my-first-pod
my-first-pod                            2/2       Running            0          2h        192.168.96.10   srv-k8s-node2

   ```
   
8. 在node节点查看容器的运行情况, 如下所示，在node节点上看到三个容器-centos，nginx，pause-为什么会是三个呢？因为需要保证每个pod有特定的linux命名空间，如果centos和nginx容器死掉，对应的命名空间也将被销毁，因此，pause容器在pod中用来维护pod的linux的命名空间；

   ```
   [root@srv-k8s-node2 ~]# docker ps | grep my-first-pod
d768d8b73af1        centos                                     "/bin/sh -c 'while : "   2 hours ago         Up 2 hours                              k8s_my-centos.1e38f164_my-first-pod_default_7d0d9f18-c1d1-11e7-8169-000c29d48309_95d9e964
dc9eaaafafcb        nginx                                      "nginx -g 'daemon off"   2 hours ago         Up 2 hours                              k8s_my-nginx.1e3ddce6_my-first-pod_default_7d0d9f18-c1d1-11e7-8169-000c29d48309_ae629be9
3d912ffc7a64        gcr.io/google_containers/pause-amd64:3.0   "/pause"                 2 hours ago         Up 2 hours                              k8s_POD.d8dbe16c_my-first-pod_default_7d0d9f18-c1d1-11e7-8169-000c29d48309_65877953
   ```   
   
9. 如果你有多个node节点，再创建pod的时候，scheduler会将其调度到另外一台合适的node上。

   
## 那么接下来聊聊replication controller

#### 什么是replication controller呢？

> replication controller用来确保用户想要运行pod的个数，如果replication controller内的某些pod由于某种原因挂掉，kubernetes会根据replication controller的设定，自动创建新的pod，最终的pod数回和用户的期望值是一样的，因此，不管你需不需要高可用的功能，我们仍然可以使用这个功能去保证pod的正常运行。
<br>

![](https://github.com/oaas/kubernetes/blob/master/chapter2/rc.png?raw=true)

##### 如上图所示, replication controller针对一个应用创建了三个pod
  * master上的controller用来管理和维护pod资源已满足它自己期望的状态，比如，这个例子期望的状态是3个pod；
  * master上的scheduler负责分发任务到正常的node节点；
  * replication controller使用selector来决定pod是否属于某个repliation controlloer,如上图所示，三个pod都归一个replication controller管理，因为他们有共同的标签project和role；


##### 为什么要用replication controller?
  1. 确保在任一时刻运行指定数目的Pod
  2. 容器重新调度
  3. 规模调整
  4. 在线升级
  5. 多发布版本跟踪
  6. 由三方面组成：一个用于创建pod的pod模版，一个期望副本数和一个用于选择被控制的pod集合的lab selector。
  7. kubectl  get replicationController -o wide查看rc的信息
  8. 建议rc只负责选择指定的pod，然后保证这个pod的数量和状态正确，调整已经运行的pod的CPU，Memory等操作应该直接更新pod本身，这是旁路控制和解藕的思想；
  9. 删除rc，不会影响所创建的pod；
  10. 适应场景：重调度，弹性伸缩，灰度发布，应用多版本release更新   


### 实操
1. 创建replication controller
   
   ```
   [root@srv-k8s-master1 ~]# cat my-first.rc.yaml
{
  "kind": "ReplicationController",
  "apiVersion": "v1",
  "metadata": {
    "name": "frontend-controller",
    "labels": {
      "state": "serving"
    }
  },
  "spec": {
    "replicas": 2,
    "selector": {
      "app": "frontend"
    },
    "template": {
      "metadata": {
        "labels": {
          "app": "frontend"
        }
      },
      "spec": {
        "volumes": null,
        "containers": [
          {
            "name": "php-redis",
            "image": "redis",
            "ports": [
              {
                "containerPort": 80,
                "protocol": "TCP"
              }
            ],
            "imagePullPolicy": "IfNotPresent"
          }
        ],
        "restartPolicy": "Always",
        "dnsPolicy": "ClusterFirst"
      }
    }
  }
}
   ```
   
2. 证实创建的rc 

   ```
   # rc名字frontend-controller, 期望2个pod， 当前2个，正在运行的是2个；
   [root@srv-k8s-master1 ~]# kubectl get rc
	NAME                  DESIRED   CURRENT   READY     AGE
	frontend-controller   2         2         2         1m
	
	# 如下所示，运行两个pod
	[root@srv-k8s-master1 ~]# kubectl get pod -l app=frontend
	NAME                        READY     STATUS    RESTARTS   AGE
	frontend-controller-8lnq8   1/1       Running   0          7m
	frontend-controller-x12p8   1/1       Running   0          7m
   ```   

3. 测试删除其中一个pod，检查kubectl是否会再创建一个新的pod

   ```
   [root@srv-k8s-master1 ~]# kubectl get pod -l app=frontend
	NAME                        READY     STATUS    RESTARTS   AGE
	frontend-controller-034mc   1/1       Running   0          31s
	frontend-controller-x12p8   1/1       Running   0          17m
	
	# 删除名为frontend-controller-034mc的pod
	[root@srv-k8s-master1 ~]# kubectl delete pod frontend-controller-034mc 
	pod "frontend-controller-034mc" deleted
	
	# 名为frontend-controller-pf26s的pod被创建
	[root@srv-k8s-master1 ~]# kubectl get pod -l app=frontend
	NAME                        READY     STATUS    RESTARTS   AGE
	frontend-controller-pf26s   1/1       Running   0          4s
	frontend-controller-x12p8   1/1       Running   0          17m
	
   ```


## 再谈services
#### 什么是service？

> service,简单来说就是个负载均衡器，客户端不需要知道后端应用pod的真实ip地址或端口，直接访问service，service负责分发流量到后端运行的pods；

![](https://github.com/oaas/kubernetes/blob/master/chapter2/service.png?raw=true)

##### 如上图所示, 
* 和replication controller一样，service通过标签将pod连到一起；service的selector用来指定连接哪些标签的pod；
* 到达service的请求会被分发到四个pod上；
* replication controller确保运行的pod个数是它自己期望的状态，它会监控pod的状态，确保后端的pod正常向serivce提供服务；

#### 为什么要用service？
  1. 由于重新调度等原因，pod在Kubernetes中的IP地址是不固定的，因此需要一个代理来确保需要使用pod的应用不需要知晓pod的真实ip地址，另一个原因是当使用replication controller创建了多个pod的副本时，需要一个代理来为这些pod做负载均衡。
  2. 由一个IP地址和一个label selector组成，在创建之初，每个services便被分配了一个独立的IP地址，该IP地址与service的生命周期相同且不能更改
  3. label标签是可选的，没有label标签的service作用是代理一些并非pod，或者得不到label的资源，比如访问集群外部的数据库
  4. 抽象一系列Pod并定义其访问规则
  5. 固定IP地址和DNS发现服务
  6. 通过环境变量和DNS发现服务
  7. 负载均衡
  8. 外部服务 ---ClusterIP, NodePort, LoadBalancer

  
#### 实操
###### 在创建之前，有两个配置我们需要注意；一个是标签，另一个是端口，正如之前提到的，servcie和pod有他们自己的标签，也正是通过标签联系到一块，因此请确保应用正确的标签和端口;

##### 创建一个和下图一样的service:

> kubectl expose pod $POD_NAME --labels="Name=Amy-log-service" --selector="App=LogParser, Owner=Amy" --port=8080 --target-port=80

> > --labels: key=value形式，用来给service指定标签

> > --selector: 选择匹配标签的资源, 如果没有指定，即指定资源的标签
> > 
> > --port: 用来指定service暴露的端口，如果没有，即容器的端口
> > 
> > --target-port: 指定用来连接的容器端口，如果没有，即--port指定的端口


![](https://github.com/oaas/kubernetes/blob/master/chapter2/service_pod.png?raw=true)


1. 创建

   ```
   # 创建名字为nginx-pod的pod
   [root@srv-k8s-master1 ~]# kubectl run nginx-pod --image=nginx --port=80 --restart="Never" --labels="app=nginx"
	pod "nginx-pod" created
	
	# 创建名字为service-pod的service, 并与nginx-pod的pod关联
	[root@srv-k8s-master1 ~]# kubectl  expose pod nginx-pod --port=8000 --target-port=80 --name="service-pod"
	service "service-pod" exposed
	
	# 查看pod
	[root@srv-k8s-master1 ~]# kubectl get pod -l app=nginx
	NAME        READY     STATUS    RESTARTS   AGE
	nginx-pod   1/1       Running   0          3m
	
	#查看services
	[root@srv-k8s-master1 ~]# kubectl get services
	NAME                     CLUSTER-IP        EXTERNAL-IP   PORT(S)        AGE
	service-pod              192.168.225.70    <none>        8000/TCP       24s
   ```
  
 2. 访问service的ip+端口, 检查是否可以获取nginx的页面（我目前测试不成功）
        
    ```
    [root@srv-k8s-master1 ~]# curl http://192.168.225.70:8000
    
    ```
    
#### 采用不同的类型去创建service
> 有三种类型的service： ClusterIP，NodePort和LoadBalancer

1. ClusterIP: 默认情况，service会被随机分配一个内网IP地址
2. NodePort：包含ClusterIP类型的service的功能，允许用户在每台node节点通过同样的端口去暴露一个service
3. LoadBalancer：包含 前两种类型的功能，于此同时，我们可以绑定云提供商的load balancer IP给这种类型的service

![](https://github.com/oaas/kubernetes/blob/master/chapter2/service_type.png?raw=true)

#####创建一个NodePort类型的service

	```
	# 1.  replication controller的配置
	[root@srv-k8s-master1 ~]# cat rc.yaml
	apiVersion: v1
	kind: ReplicationController
	metadata:
	  name: nginx
	spec:
	  replicas: 3
	  selector:
	    app: nginx
	  template:
	    metadata:
	      name: nginx
	      labels:
	        app: nginx
	    spec:
	      containers:
	      - name: nginx
	        image: nginx
	        ports:
	        - containerPort: 80
   
    # 2. 创建rc
	[root@srv-k8s-master1 ~]# kubectl create -f rc.yaml
	replicationcontroller "nginx" created    
	
	# 3. 等候几分钟，看rc的状态，READY的个数为3，说明创建完成
	[root@srv-k8s-master1 ~]# kubectl get rc
	NAME                  DESIRED   CURRENT   READY     AGE
	frontend-controller   2         2         2         5h
	nginx                 3         3         3         1m
	
	# 4. 创建NodePort类型的Service
	[root@srv-k8s-master1 ~]# kubectl expose rc nginx --name=service-nodeport --type="NodePort"
	service "service-nodeport" exposed
	
	# 5. 查看service的状态， 如下所示，NodePort的端口会被kubernetes在30000到32767之间随机生成，注意端口是被暴露在node节点上, 
	[root@srv-k8s-master1 ~]# kubectl describe svc service-nodeport
	Name:			service-nodeport
	Namespace:		default
	Labels:			app=nginx
	Selector:		app=nginx
	Type:			NodePort
	IP:			192.168.26.26
	Port:			<unset>	80/TCP
	NodePort:		<unset>	31293/TCP
	Endpoints:		192.168.96.18:80,192.168.96.19:80,192.168.96.20:80
	Session Affinity:	None
    No events.
	
    #  6. 通过Node_IP + 31293访问；
    [root@srv-k8s-node2 ~]# curl -I 10.1.96.233:31293
    HTTP/1.1 200 OK
	Server: nginx/1.13.6
	Date: Sun, 05 Nov 2017 11:46:10 GMT
	Content-Type: text/html
	Content-Length: 612
	Last-Modified: Thu, 14 Sep 2017 16:35:09 GMT
	Connection: keep-alive
	ETag: "59baafbd-264"
	Accept-Ranges: bytes
	
	# 如何工作的呢？ flanneld根据我们之前配置的网络环境，分配网络地址给pod，于此同时存储网络信息到etcd，kube-proxy担任service和pods之间的网络连接桥梁；
	```
	

## 再谈volumes
#### 为什么会有volume?

###### 文件在容器中的生命周期是短暂的，伴随着容器的销毁而销毁. Docker本身提供一些机制比如挂载宿主机volume，或volume容器去帮助我们去解决文件的持久化问题，但是通过这些方式很难实现夸主机的文件共享，存储， kubernete引入了volumes的概念；

* 数据持久化
* Pod内容器共享数据
* 生命周期：跟随pod的生命周期
* 支持多种类型的数据卷：

  ![](https://github.com/oaas/kubernetes/blob/master/chapter2/volume_provider.png?raw=true)

#### 怎么使用这些volume呢？
1. emptyDir: 最简单的volume类型，这种方式会创建为pod内的容器创建一个空的volume, 当pod被删除时，emptyDir内的文件也会被删除；

   ```
	[root@srv-k8s-master1 ~]# cat emptydir.yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: centos
	spec:
	  containers:
	  - image: centos
	    command:
	    - sleep
	    - "3600"
	    name: centos
	    volumeMounts:
	    - mountPath: /cache
	      name: cache-volume
	  volumes:
	  - name: cache-volume
	    emptyDir: {}
	[root@srv-k8s-master1 ~]# kubectl create -f emptydir.yaml
	 pod "centos" created  
   
   # 容器正常运行后，找到容器的运行node节点，选择容器的id，通过docker inspect $container_id， 会看到一些mount的选项
   
   # 容器ID为5c3ad84974ce
   [root@srv-k8s-node2 ~]# docker inspect 5c3ad84974ce  
   "Mounts": [
            {
                "Source": "/var/lib/kubelet/pods/ad6bee32-c22a-11e7-a9ca-000c29d48309/volumes/kubernetes.io~empty-dir/cache-volume",
                "Destination": "/cache",
                "Mode": "Z",
                "RW": true,
                "Propagation": "rprivate"
            },
            .....
            
   ```
 
2. hostPath: 类似于docker的data volume， 将会挂载node的本地的volume到容器中，注意pod有可能会被调度到其它的node上，因此在写hostPath的时候要注意，使用hostPath，你可以在容器和node之间读写文件，其文件会永久性留在node节点上；
	 
	 ```
	[root@srv-k8s-master1 ~]# cat hostpath.yaml
	apiVersion: v1
	kind: Pod
	metadata:
	  name: centos-hostpath
	spec:
	  containers:
	  - image: centos
	    name: centos-hostpath
	    command:
	    - sleep
	    - "3600"
	    volumeMounts:
	    - mountPath: /test-pd
	      name: test-volume
	  volumes:
	  - name: test-volume
	    hostPath:
	      # directory location on host
	      path: /data
	[root@srv-k8s-master1 ~]# kubectl create -f hostpath.yaml
	pod "centos-hostpath" created
	 
	 ``` 

3. 由于环境限制，其它不一一做展示，有需要请查看官网；

## 关于secrets

#### 什么是secrets？

###### secrets，顾名思义，用来传递敏感的信息给pod，kubernetes secrets通过key-value的方式（必须是用base64编码的）管理信息，正确的使用secrets可以减少必要的风险，还可以更加高效的组织我们的资源。

##### 有三种类型的secret
> 注意: secrets文件有1MB的限制，类似于volume，需要实现创建好，才能在pod中使用
1. opaque： 默认的类型
2. service account token
3. docker authentication

```
# 1. 准备secret yaml文件
[root@srv-k8s-master1 ~]# cat secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
 
# 2. 创建secret  
[root@srv-k8s-master1 ~]# kubectl create -f ./secret.yaml
secret "mysecret" created  

# 3. 无法查看用户名密码信息
[root@srv-k8s-master1 ~]# kubectl describe secrets mysecret
Name:		mysecret
Namespace:	default
Labels:		<none>
Annotations:	<none>

Type:	Opaque

Data
====
password:	12 bytes
username:	5 bytes

# 4. 准备pod, 挂载secret
[root@srv-k8s-master1 ~]# cat my-secret.pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysecret
spec:
  containers:
  - name: mysecret
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
 
# 5. 创建secret测试pod      
[root@srv-k8s-master1 ~]# kubectl create -f my-secret.pod.yaml
pod "mysecret" created

# 6. 登陆到对应的docker容器内，查看secrets指定的信息
[root@srv-k8s-node2 ~]# docker exec -it 2ce234708ec9 bash
root@mysecret:~# cat /etc/foo/username
admin
root@mysecret:~# cat /etc/foo/password
1f2d1e2e67df

# 这是怎么实现的呢？ 为了降低风险，kubernetes不会存储这些敏感信息到磁盘上，secrets信息存储在内存当中，更精确的说是，kubernetes API server推送secret到对应容器运行的node节点，node存放这些信息在tmpfs中，如果容器被销毁，这些信息也会被清楚掉；因此建议不要存放过多的信息到secret中去，否则会影响系统性能；

[root@srv-k8s-node2 ~]# df -h --type=tmpfs
Filesystem      Size  Used Avail Use% Mounted on
tmpfs           1.9G     0  1.9G   0% /dev/shm
tmpfs           1.9G  154M  1.7G   9% /run
tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup
tmpfs           378M     0  378M   0% /run/user/0
tmpfs           1.9G  8.0K  1.9G   1% /var/lib/kubelet/pods/bbb288c1-c231-11e7-a9ca-000c29d48309/volumes/kubernetes.io~secret/foo
```




	
	
	




